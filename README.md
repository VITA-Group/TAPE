<h1 align="center">Rethinking Addressing in Language Models via Contextualized Equivariant Positional Encoding (TAPE)</h1>
<p align="center">
    <a href="https://arxiv.org/abs/2501.00712"><img src="https://img.shields.io/badge/arXiv-2501.00712-B31B1B?labelColor=white&logo=data:image/vnd.microsoft.icon;base64,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" alt="arXiv"></a>
    <a href="https://icml.cc/virtual/2025/poster/43612"> <img alt="ICML25" src="https://img.shields.io/badge/Pub-ICML%272025-red?logoSize=auto&labelColor=black&logo=data:image/vnd.microsoft.icon;base64,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" alt="OpenReview"></a>
    <a href="https://github.com/VITA-Group/TAPE"><img src="https://img.shields.io/github/stars/VITA-Group/TAPE"></a>
</p>


This repository contains the official implementation of TAPE as described in the paper: [Rethinking Addressing in Language Models via Contextualized Equivariant Positional Encoding](https://arxiv.org/abs/2501.00712) by Jiajun Zhu, Peihao Wang, Ruisi Cai, Jason D. Lee, Pan Li, Zhangyang Wang.


## Getting Started
```shell
conda create -n adape python=3.10
conda activate adape
pip install -r requirements.txt
pip install flash-attn --no-build-isolation
```

## Arithmetic Learning
It is implemented independently under the directory `arithmetic/`, another github repo linked as submodule. It should has independent environment as well. Please refer to its [README](https://github.com/zhuconv/arithmetic/blob/main/README.md) for detailed instructions about training and evaluation.

## Training From Scratch
### Pretraining
The scripts under script/ covers the commands for training. For example, you can start training TAPE (`adape` in code) model with the following command:

```shell
export TYPE=adape
bash script/train.sh
```
You can change CONFIG_NAME to choose different positional encoding variants. (`choose from those under config/`)

### Finetuning on SCROLLS and Evaluation
There are three steps to get evaluation results:
1. finetune pre-trained models on SCROLLS
2. generate answers in validation set
3. evaluate the answers with corresponding metric

```shell
export TYPE=adape DATASET_NAME=quality
export METRIC_DIR=scrolls/metrics
export SAVE_DIR=scrolls/quality
bash script/ft_scrolls.sh # assume the pretrained checkpoint is under output/${TYPE}_c4, if not, need to set 'output_name=<your_output_name>'
bash script/gen_scrolls.sh
python eval_scrolls.py --split validation --dataset_name $DATASET_NAME --predictions ${SAVE_DIR}/${TYPE}.json  --metrics_output_dir $METRIC_DIR
```

You can change DATASET_NAME to choose different dataset. (`choose from ['narrative_qa', 'quality', "qasper", 'contract_nli']`)

## PEFT Llama2-7B
### Finetuning
Similiar to training from scratch, you can use the following command ans select different methods: 
```shell
export TYPE=adape
bash script/train_llama.sh
```

### Evaluation
For finetuning perplexity evaluation, you need to manually download data hosted by [LongLoRA](https://github.com/dvlab-research/LongLoRA/tree/main)

| Dataset    | Split      | Link                                                                                                         |
|:-----------|------------|--------------------------------------------------------------------------------------------------------------|
| PG19       | test       | [pg19/test.bin](https://drive.google.com/file/d/1QANDMdctpacPAYgS04adDXqByGEq-Ret/view?usp=share_link)       |
| Proof-pile | test       | [proof-pile/test_sampled_data.bin](https://drive.google.com/file/d/1bUI5lPDvrqzY_XXJJ2sSuvZx0Y9AZClE/view?usp=share_link)         |
 
 Then you can use the following command:
```shell
data=proof_pile
model_path=output/llama_adape
bash script/eval_llama.sh
```

We also have `eval_retrieval.py` for evaluation on passkey retrieval task.
```shell
python3 eval_retrieval.py --context_size 8192 --base_model output/llama_adape --max_tokens 8192 --interval 1000
```


## Credits
The codebase are inherited from [BiPE](https://github.com/zhenyuhe00/BiPE) and [LongLoRA](https://github.com/dvlab-research/LongLoRA/tree/main). Thanks to their excellent work!
