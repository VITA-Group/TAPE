<h1 align="center">Rethinking Addressing in Language Models via Contextualized Equivariant Positional Encoding (TAPE)</h1>
<p align="center">
    <a href="https://arxiv.org/abs/2501.00712"><img src="https://img.shields.io/badge/arXiv-2501.00712-B31B1B?labelColor=white&logo=data:image/vnd.microsoft.icon;base64,AAABAAEAMDAAAAAAIACMBgAAFgAAAIlQTkcNChoKAAAADUlIRFIAAAAwAAAAMAgGAAAAVwL5hwAAAARnQU1BAACxjwv8YQUAAAABc1JHQgCuzhzpAAAAIGNIUk0AAHomAACAhAAA+gAAAIDoAAB1MAAA6mAAADqYAAAXcJy6UTwAAAYKSURBVGiBzVldbBRVFP7OzG7LtqVAq0aawoDdNUZQEStEfSAGH4iwA4Vtqey2kUDSEDQxGH0AakXABGM0UUM0SkxgIXZbTDsL1Zgo/hED1mDCg5EuoVuxyk8rhcK2y8wcH9ol7e7M7s7Oonxve865537f3Hv3njlDSIOw5L4KoqnM+j45enZjutj/C2TmUCR3LxFJtwzMX3ujkWf+E1YWIJg5iMGTDbRUkdwnbjsjizAV0N0XqUq2EdEiZbb71O2lZA2mAnYAuqZiSbKdBFoQltzR20sre5iegQQ6Z7t3CQJtS7Yz87AcjUzNFxGlLbiXiDYxM/81eL2wqanpZjbjTFcggZV9ke3MHEm2E1GJInmGciGbDKU9+DMRbRrPSzPLigezHZtRAADI0YiHwSPJdiKUhiXPleyppkJpC54hUPWkvMCxbMdn3EITEZbcGohSRTPHu6MR1w5At5JPaQsOEtGMyblwzVvrL802hyUBn1dUlDsLii+b+WO9PY46QMuUp6WlRah+yBMHSExy6V6fP9mWFlltoQRW9/cPgLXlZn6X5B7NlKOrq6vwsfke1YA8VDFeboUPYHEFElCkqk+IhA2GTmYtFo0UGq1ER8ehClHlP42GaUwLV9Wus3zH5CQAAMKSOwqi2UY+ZuaRG0OldZcuDSdsR0LBBSyQIUGCvn2Fr2F3LjxyFgAAYck9CqIC0+T6aNmKvr5/Olv3LxNE8QvDIOZvvLWBpblysHQGktEdjRSn87NQOBj+6P23zMgz84Ad8oDNFQCAoxVz79cLHL+bBjADzVs0TJ+Wcmi9Pr8AJBWNFmFrBQBgef+5MzpjvWkAEbDzHZH/vnjrUDMz9w8MF8AmeSAPK5BAWKr6FiSkFH8TwS9u1EiqFFUxPqOmZr2tGzyBvAkAAEXyXCTC3aYBzODFC5vlUGhXvua0vYUmgPB2SymYzcsJItDJUzs7Z933fP4mzQNaW1tdRaI6DEDg0VGVtr7pAKVPzcyb5Whkr925LdUdRjh27NMpPEIxjD8McjgEVD+s4YcTQjoRRLS8vrQs9tnQ4HE789veQsMDztTavbxc5DUrMr6QCALtUWbOLLIzvy0BzEwAuYx89OTjTp73gJoxh7PoYzscbAk4cOBA2qdH6+sdKJ2atrwmoko7HGwJaGxsvJ4x6LWXRU53YTHes8PB9hlgoCZj0J7msZIieSyzKvdFDtuZ37YA2efv0Flfmy6GRJGwfcukrcTM6i/RSKHd+fN2E3ce3r9MYJOSeRz8U7dKh484mHlQjkYsv30ZIW838co1jV9q3x0PpIuhJ6od/Mi8/nyRB/JbC5Eyx6NR2XQdW19Ke0FqV2KlqzZuvJaXSfORBAApkjtORA4A4JJinV5/Jd3q6jHdWVBXV5exg5EJedlCiuS+nCAPADR8XcDW3TqzwV/P+LwuIZ6xg5ENbAsIz3EfIqKyFEf8pkCvvsEwbXaRqLQd7LE7vy0B4UpPPUDPmQaoWk1MdxqWGgBABLfSfnCfHQ45n4HWysqyIodrwMx/U1UXrz5/7iQAhEIhcQrFR4lSm1kAwCpkud4fzoVHTgJCgOiS3HHDPikANRafW3Mh2ptsV9qDN8ik+BO1kXueXbvhklUuOW0h1xyPIXlm5itXeZoReQCQfYEi5tQuNwBo4pSLoVDI8vuJZQHj38kMx8W0keKGwcjVdOPl2oAL4LiRzyXEb1jlY0mAInm2EdGiZDszqyW9Pc6158/Hssnj9QUKAb6Q6qECpS142gonSwKIkNpNGPvU5HwayPjyMhFeX+BeZv4tdQ6a39kW/DDbPFkL6JhVJacYGUNeG9/J5NrAg8z4NYUUUdPR9uBT2eTIWsCpP84emfibmQe80Z7p2Y43g1zrfxSM75PtOujHrtZ95j2mcWQtYAegazrPY+bT0PV35WjkLqtkzeCt9S8B663JdlUovDD23m2OvHbm7CIcOvgBBGyeaGPwsOwLmG7TfHbmbMNb53+BQc0TbQQqUdqDX5mNuaMEAIDsW7dLZ71hoo1AC8zi7zgBALCytiGos7pq7BdrMd1p2nr5F0kkJXaCJOhmAAAAAElFTkSuQmCC" alt="arXiv"></a>
    <a href="https://openreview.net/forum?id=Us1RXG1Ji2&noteId=Us1RXG1Ji2"> <img alt="OpenReview" src="https://img.shields.io/badge/ICLR%2725-OpenReview-blue?logoSize=auto&labelColor=white&logo=data:image/vnd.microsoft.icon;base64,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" alt="OpenReview"> </a>
    <a href="https://github.com/VITA-Group/TAPE"><img src="https://img.shields.io/github/stars/VITA-Group/TAPE"></a>
</p>


This repository contains the official implementation of TAPE as described in the paper: [Rethinking Addressing in Language Models via Contextualized Equivariant Positional Encoding](https://arxiv.org/abs/2501.00712) by Jiajun Zhu, Peihao Wang, Ruisi Cai, Jason D. Lee, Pan Li, Zhangyang Wang.


## Getting Started
```shell
conda create -n adape python=3.10
conda activate adape
pip install -r requirements.txt
pip install flash-attn --no-build-isolation
```

## Arithmetic Learning
It is implemented independently under the directory `arithmetic/`, another github repo linked as submodule. It should has independent environment as well. Please refer to its [README](https://github.com/zhuconv/arithmetic/blob/main/README.md) for detailed instructions about training and evaluation.

## Training From Scratch
### Pretraining
The scripts under script/ covers the commands for training. For example, you can start training TAPE (`adape` in code) model with the following command:

```shell
export TYPE=adape
bash script/train.sh
```
You can change CONFIG_NAME to choose different positional encoding variants. (`choose from those under config/`)

### Finetuning on SCROLLS and Evaluation
There are three steps to get evaluation results:
1. finetune pre-trained models on SCROLLS
2. generate answers in validation set
3. evaluate the answers with corresponding metric

```shell
export TYPE=adape DATASET_NAME=quality
export METRIC_DIR=scrolls/metrics
export SAVE_DIR=scrolls/quality
bash script/ft_scrolls.sh # assume the pretrained checkpoint is under output/${TYPE}_c4, if not, need to set 'output_name=<your_output_name>'
bash script/gen_scrolls.sh
python eval_scrolls.py --split validation --dataset_name $DATASET_NAME --predictions ${SAVE_DIR}/${TYPE}.json  --metrics_output_dir $METRIC_DIR
```

You can change DATASET_NAME to choose different dataset. (`choose from ['narrative_qa', 'quality', "qasper", 'contract_nli']`)

## PEFT Llama2-7B
### Finetuning
Similiar to training from scratch, you can use the following command ans select different methods: 
```shell
export TYPE=adape
bash script/train_llama.sh
```

### Evaluation
For finetuning perplexity evaluation, you need to manually download data hosted by [LongLoRA](https://github.com/dvlab-research/LongLoRA/tree/main)

| Dataset    | Split      | Link                                                                                                         |
|:-----------|------------|--------------------------------------------------------------------------------------------------------------|
| PG19       | test       | [pg19/test.bin](https://drive.google.com/file/d/1QANDMdctpacPAYgS04adDXqByGEq-Ret/view?usp=share_link)       |
| Proof-pile | test       | [proof-pile/test_sampled_data.bin](https://drive.google.com/file/d/1bUI5lPDvrqzY_XXJJ2sSuvZx0Y9AZClE/view?usp=share_link)         |
 
 Then you can use the following command:
```shell
data=proof_pile
model_path=output/llama_adape
bash script/eval_llama.sh
```

We also have `eval_retrieval.py` for evaluation on passkey retrieval task.
```shell
python3 eval_retrieval.py --context_size 8192 --base_model output/llama_adape --max_tokens 8192 --interval 1000
```


## Credits
The codebase are inherited from [BiPE](https://github.com/zhenyuhe00/BiPE) and [LongLoRA](https://github.com/dvlab-research/LongLoRA/tree/main). Thanks to their excellent work!
