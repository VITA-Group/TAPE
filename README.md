<h1 align="center">Rethinking Addressing in Language Models via Contextualized Equivariant Positional Encoding (TAPE)</h1>
<p align="center">
    <a href="https://arxiv.org/abs/2501.00712"><img src="https://img.shields.io/badge/arXiv-2501.00712-B31B1B?labelColor=white&logo=data:image/vnd.microsoft.icon;base64,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" alt="arXiv"></a>
    <a href="https://icml.cc/virtual/2025/poster/43612"> <img alt="ICML25" src="https://img.shields.io/badge/Pub-ICML%272025-red?logoSize=auto&labelColor=black&logo=data:image/vnd.microsoft.icon;base64,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" alt="OpenReview"></a>
    <a href="https://github.com/VITA-Group/TAPE"><img src="https://img.shields.io/github/stars/VITA-Group/TAPE"></a>
</p>


This repository contains the official implementation of TAPE as described in the paper: [Rethinking Addressing in Language Models via Contextualized Equivariant Positional Encoding](https://arxiv.org/abs/2501.00712) by Jiajun Zhu, Peihao Wang, Ruisi Cai, Jason D. Lee, Pan Li, Zhangyang Wang.


## Getting Started
```shell
conda create -n adape python=3.10
conda activate adape
pip install -r requirements.txt
pip install flash-attn --no-build-isolation
```

## Arithmetic Learning
It is implemented independently under the directory `arithmetic/`, another github repo linked as submodule. It should has independent environment as well. Please refer to its [README](https://github.com/zhuconv/arithmetic/blob/main/README.md) for detailed instructions about training and evaluation.

## Training From Scratch
### Pretraining
The scripts under script/ covers the commands for training. For example, you can start training TAPE (`adape` in code) model with the following command:

```shell
export TYPE=adape
bash script/train.sh
```
You can change CONFIG_NAME to choose different positional encoding variants. (`choose from those under config/`)

### Finetuning on SCROLLS and Evaluation
There are three steps to get evaluation results:
1. finetune pre-trained models on SCROLLS
2. generate answers in validation set
3. evaluate the answers with corresponding metric

```shell
export TYPE=adape DATASET_NAME=quality
export METRIC_DIR=scrolls/metrics
export SAVE_DIR=scrolls/quality
bash script/ft_scrolls.sh # assume the pretrained checkpoint is under output/${TYPE}_c4, if not, need to set 'output_name=<your_output_name>'
bash script/gen_scrolls.sh
python eval_scrolls.py --split validation --dataset_name $DATASET_NAME --predictions ${SAVE_DIR}/${TYPE}.json  --metrics_output_dir $METRIC_DIR
```

You can change DATASET_NAME to choose different dataset. (`choose from ['narrative_qa', 'quality', "qasper", 'contract_nli']`)

## PEFT Llama2-7B
### Finetuning
Similiar to training from scratch, you can use the following command ans select different methods: 
```shell
export TYPE=adape
bash script/train_llama.sh
```

### Evaluation
For finetuning perplexity evaluation, you need to manually download data hosted by [LongLoRA](https://github.com/dvlab-research/LongLoRA/tree/main)

| Dataset    | Split      | Link                                                                                                         |
|:-----------|------------|--------------------------------------------------------------------------------------------------------------|
| PG19       | test       | [pg19/test.bin](https://drive.google.com/file/d/1QANDMdctpacPAYgS04adDXqByGEq-Ret/view?usp=share_link)       |
| Proof-pile | test       | [proof-pile/test_sampled_data.bin](https://drive.google.com/file/d/1bUI5lPDvrqzY_XXJJ2sSuvZx0Y9AZClE/view?usp=share_link)         |
 
 Then you can use the following command:
```shell
data=proof_pile
model_path=output/llama_adape
bash script/eval_llama.sh
```

We also have `eval_retrieval.py` for evaluation on passkey retrieval task.
```shell
python3 eval_retrieval.py --context_size 8192 --base_model output/llama_adape --max_tokens 8192 --interval 1000
```


## Credits
The codebase are inherited from [BiPE](https://github.com/zhenyuhe00/BiPE) and [LongLoRA](https://github.com/dvlab-research/LongLoRA/tree/main). Thanks to their excellent work!
